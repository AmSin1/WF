# This workflow is configired to be trigerred manually from GitHUb WebUI or by an external cronjob scheduler service
# This GitHub Action has 3 jobs:
# 1) copies specified playlists from source repo and overwrites corresponding specified playlists in destination branch/directory in destination repo
# 2) creates a custom-formatted deduplicated merged playlist with prioritized m3u comtent-blocks from already copied source playlists
# 3) echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
# Works for copying from any branch of a public source repo into any branch of a public/private destination repo
# The .yml file may be placed in any 3rd public/private repo (non-source, non-destination)(say, control repo) in its default (i.e. main) branch at the 'github/workflows/' directory
# Need to use Personal Access Token (PAT) for commiting and pushing to destination repo - create a PAT Classic (with workflow permissions) or PAT Fine-grained (with actions(R/W), contents(code)(R/W), pull requests(R/W) and workflows(R/W) permissions)
# Store PAT Classic or PAT Fine-grained token's value in the control repo's secrets with names PAT_CL_SYNC or PAT_FG_SYNC

name: FileSync SL&FC LE 6

on: 
  workflow_dispatch: # allows manual workflow run from the Actions tab (workflow must be in default branch to work)
    inputs: # optional input(s) that the cronjob service can pass on to the workflow
      message:
        description: 'source trigger of the workflow run'
        required: true
        default: 'manual GitHub WebUI trigger' # default message to be printed if workflow is triggered manually via WebUI or if there is no input message alongside a trigger by cronjob service
        type: string # optionally, specifies type of message
  # internal scehduled run parameters in the 2 lines below 'commented out' as external cronjob scheduler service is engaged
  # schedule: # scheduled to run at designated times or every x minutes (minimum interval allowed is 5 minutes)
    # - cron: '*/22 * * * *'  
      
env:
  GITHUB_TOKEN: ${{ secrets.PAT_FG_SYNC }}
  CRONJOB_SERVICE: "FastCron"
  DESTINATION_REPO_URL: "https://github.com/AmSin1/Repo.git"
  DESTINATION_REPO: "AmSin1/Repo"
  DESTINATION_BRANCH: "autocopied"
  DESTINATION_DIR1_PATH: "sl-le"
  MERGED_PLAYLIST1: "sl-le-all.m3u"
  SOURCE_PLAYLIST1a_URL: "https://github.com/doctor-8trange/zyphora/raw/main/data/sony.m3u" # source updated and working
  SOURCE_PLAYLIST1b_URL: "https://github.com/drmlive/sliv-live-events/raw/main/sonyliv.m3u" # sporce updated and working
  SOURCE_PLAYLIST1c_URL: "https://github.com/abusaeeidx/SonyLiv-Event-Auto-update/raw/main/sonyliv.m3u" # source NOT updated
  SOURCE_PLAYLIST1d_URL:
  DESTINATION_DIR2_PATH: "fc-le"
  MERGED_PLAYLIST2: "fc-le-all.m3u"
  SOURCE_PLAYLIST2a_URL: "https://github.com/jitendra-unatti/fancode/raw/main/data/fancode.m3u" # source updated and working
  SOURCE_PLAYLIST2b_URL: "https://github.com/drmlive/fancode-live-events/raw/main/fancode.m3u" # source updated and working
  SOURCE_PLAYLIST2c_URL: "https://github.com/IPTVFlixBD/Fancode-BD/raw/main/playlist.m3u" # source updated and working
  SOURCE_PLAYLIST2d_URL: "https://github.com/abusaeeidx/CrestSport-IPTV-Collection/raw/main/fncbd.m3u" # source updated and working
  SOURCE_PLAYLIST2e_URL:

concurrency: # ensures that only one GitHub workflow runs at a time for a specific branch in a repo, prevents conditions where two separate workflow runs attempt to push to same branch simultaneously
  group: sync-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: false # 'false' ensures that a newly-started workflow runs sequentially after and does not terminate a still in-progress workflow run, ensuring that every workflow's changes are eventually pushed

jobs:   
  copy_source_playlists: # copies specified playlists from source repos and overwrites corresponding specified playlists in destination branch/directories in destination repo
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:
      - name: Checkout Destination Branch in Destination Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DESTINATION_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DESTINATION_BRANCH }}  # set the destination branch to checkout
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'destination-branch'

      - name: Download Playlists from Source Repos  
        run: |
          START_TIME=$(date +%s) # records the start time of this run in seconds (for comparison)
          mkdir -p destination-branch/${{ env.DESTINATION_DIR1_PATH }} # creates destination directory if it doesn't already exist
          mkdir -p destination-branch/${{ env.DESTINATION_DIR2_PATH }} # creates destination directory if it doesn't already exist

          # Step 1: Define the playlist array
          PLAYLISTS=( # defines the list of playlists to process in a pipe-separated array (URL|DIR|FILE)
            "${{ env.SOURCE_PLAYLIST1a_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-1.m3u"
            "${{ env.SOURCE_PLAYLIST1b_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-2.m3u"
            "${{ env.SOURCE_PLAYLIST1c_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-3.m3u"
            "${{ env.SOURCE_PLAYLIST2a_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-1.m3u"
            "${{ env.SOURCE_PLAYLIST2b_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-2.m3u"
            "${{ env.SOURCE_PLAYLIST2c_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-3.m3u"
            "${{ env.SOURCE_PLAYLIST2d_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-4.m3u"
          )

          # Step 2: Download playlists using curl command
          # Optional Curl configuration in next line:
          CURL_FLAGS="--retry 3 --retry-delay 1 --max-time 6 --no-progress-meter" # retry up to x times; delay of exactly x seconds between retries; total time for initial attempt + retries capped at x seconds
          for entry in "${PLAYLISTS[@]}"; do # iterates through the playlists array
            IFS="|" read -r URL DIR FILE <<< "$entry" # extracts data from the playlist array string using IFS pipe separator 
            TARGET="destination-branch/$DIR/$FILE" # sets destination file path for each playlist download 
       
            # Sub-step 2a: Fetch the playlist's last update date and download playlist 





            echo "------------------------------------------------------------"
            echo "DEBUG: Processing $FILE"

            # 1. Extract components using a more reliable sed/read combo
            # This handles both ://github.com... and raw.githubusercontent.com URLs
            OWNER_REPO_PATH=$(echo "$URL" | sed -E 's|https://github.com([^/]+)/([^/]+)/raw/([^/]+)/(.*)|\1 \2 \3 \4|')
            read -r OWNER REPO BRANCH FILE_PATH <<< "$OWNER_REPO_PATH"
            
            echo "DEBUG: Parsed - Owner: $OWNER, Repo: $REPO, Branch: $BRANCH, Path: $FILE_PATH"

            # 2. Build API URL with absolute pathing to avoid variable truncation
            if [ -n "$OWNER" ] && [ -n "$REPO" ]; then
              # Construct the full API endpoint for the last commit on that file path
              FULL_API_URL="https://api.github.com{OWNER}/${REPO}/commits?path=${FILE_PATH}&sha=${BRANCH}&per_page=1"
              echo "DEBUG: Full API URL: $FULL_API_URL"
              
              # 3. Fetch API response with authentication
              API_RESPONSE=$(curl -s -H "Authorization: Bearer ${{ secrets.PAT_FG_SYNC }}" "$FULL_API_URL")
              
              # 4. Extract date from the FIRST object of the commit array
              # Using '[0]' explicitly fixes the "Cannot index object with number" error
              RAW_COMMIT_DATE=$(echo "$API_RESPONSE" | jq -r 'if type=="array" then .[0].commit.committer.date else empty end')
              echo "DEBUG: Extracted Date: $RAW_COMMIT_DATE"
            else
              RAW_COMMIT_DATE=""
              echo "DEBUG ERROR: Parsing failed for URL: $URL"
            fi

            # 5. Convert to IST
            if [ -n "$RAW_COMMIT_DATE" ] && [ "$RAW_COMMIT_DATE" != "null" ]; then
              SRC_MOD_TIME=$(TZ='Asia/Kolkata' date -d "$RAW_COMMIT_DATE" "+%d-%m-%Y %H:%M IST" 2>/dev/null || echo "Unknown")
            else
              SRC_MOD_TIME="N/A"
            fi






            





            
            curl $CURL_FLAGS -L --fail -o "$TARGET" "$URL" || true # downloads playlist; 'true' here is the file-level 'continue-on-error: true' flag to ensures that the loop continues even if one download fails

            # Sub-step 2b: Echo success/failure and key metrics for each download
            if [ -f "$TARGET" ] && [ $(date -r "$TARGET" +%s) -ge $START_TIME ]; then # checks if playlist exists AND was actually updated/created during this specific workflow run            
              SIZE=$(stat -c%s "$TARGET")  # calculates file size of such playlist
              COUNT=$(grep -c '#EXTINF' "$TARGET" || true) # counts number of #EXTINF lines in such playlist
              echo "$FILE download : SUCCESS. Last updated: $SRC_MOD_TIME. File size $SIZE bytes. $COUNT #EXTINF lines present."
            else
              echo "$FILE download : FAILED"
            fi
          done

          # Step 3: Clean up temporary work files       
          rm -f *.tmp *.temp temp_* # 
      
      - name: Commit Changes and Push to Destination Branch
        run: |
          cd destination-branch # changes current working directory to the destination branch

          # Step 1: Configure git          
          git config url.https://${{ secrets.PAT_FG_SYNC }}@github.com/.insteadOf https://github.com/
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"  

          # Step 2: Define the list of downloaded playlists for analysis
          FILES=(
            "${{ env.DESTINATION_DIR1_PATH }}/sl-le-1.m3u"
            "${{ env.DESTINATION_DIR1_PATH }}/sl-le-2.m3u"
            "${{ env.DESTINATION_DIR1_PATH }}/sl-le-3.m3u"
            "${{ env.DESTINATION_DIR2_PATH }}/fc-le-1.m3u"
            "${{ env.DESTINATION_DIR2_PATH }}/fc-le-2.m3u"
            "${{ env.DESTINATION_DIR2_PATH }}/fc-le-3.m3u"
            "${{ env.DESTINATION_DIR2_PATH }}/fc-le-4.m3u"
          )

          # Step 3: Iterate through the downloaded playlists to analyse using tracking variables for the final summary echos              
          COMMITTED_NAMES=""
          COMMITTED_COUNT=0
          SKIPPED_NAMES=""
          SKIPPED_COUNT=0
          SUMMARY_DETAILS=""         
          for FILE in "${FILES[@]}"; do
            FILE_NAME=$(basename "$FILE")
            if [ -f "$FILE" ]; then       
              GIT_STATUS=$(git status --porcelain "$FILE") # uses git status to determine the file state
              if [[ -z "$GIT_STATUS" ]]; then # if git status is empty, it means the file is unchanged/clean
                SKIPPED_NAMES+="$FILE_NAME " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for playlist $FILE_NAME."
              else
                COMMITTED_NAMES+="$FILE_NAME " # playlist is changed or new and will be committed; adds to committed list and gathers metrics
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SIZE=$(stat -c%s "$FILE") 
                EXTINF=$(grep -c '#EXTINF' "$FILE" || true)
                SUMMARY_DETAILS+="\n  - $FILE_NAME ($SIZE bytes): now has $EXTINF #EXTINF lines"
                git add "$FILE" # stage only the changed/new files selectively based on the git status check         
              fi
            fi
          done

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "----------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit in any playlists"
          else
            git commit -m "Automated sync from source" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DESTINATION_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DESTINATION_BRANCH }} # pushes to the destination branch
            echo "----------------------------------------------------------------------------------------------------------"
            echo "RESULT: Commits were made to $COMMITTED_COUNT playlist(s): $COMMITTED_NAMES"
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "----------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT playlist(s) were found unchanged and were skipped: $SKIPPED_NAMES"
          echo "----------------------------------------------------------------------------------------------------------"
          

  
  create_merged_playlist: # creates custom-formatted deduplicated merged playlists with prioritized m3u comtent-blocks from input playlists
    needs: copy_source_playlists # forces this job to wait for the referenced job to finish (forces sequential instead of the default parallel execution)
    if: always() # this job runs regardless of other jobs' success/failure status
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:
      - name: Checkout Destination Branch in Destination Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DESTINATION_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DESTINATION_BRANCH }}  # set the destination branch to checkout
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'destination-branch'

      - name: Process, Deduplicate, Format and Merge Playlists sequentially
        run: |
          cd destination-branch # changes current working directory to the destination branch
          TOTAL_GROUP_FAILURES=0 # tracks global failures across the clean and merge processes of all merged playlists, starts from 0

          # Step 1: Use helper function for seqnuential cleaning and merging
          process_group() { # function encapsulating cleaning and merging processes for any given merged playlist folder
            local dir="$1"
            local output="$2"
            shift 2
            local sources=("$@")
            
            echo "-----------------------------------------------------------------------------"
            echo "PROCESS START: Creation of merged playlist $output in folder $dir"
            echo "-----------------------------------------------------------------------------"

            # Sub-step 1a: Clean input playlists for a specific merged playlist - remove carriage returns and BOMs
            echo "Starting cleaning of input playlists for merged playlist $output..."
            local successful_san_count=0 # count of successfully cleaned input playlists for this merged playlist, starts from 0
            local cleaned_details="" # record of file-size and count of #EXTINF lines in successfully cleaned input playlists, starts blank

            for src in "${sources[@]}"; do
              local src_path="$dir/$src"
              if (set -e; [ -f "$src_path" ] && sed -i '1s/^\xef\xbb\xbf//; s/\r//g' "$src_path"); then # strips Windows line endings (\r) and any non-standard character markers
                successful_san_count=$((successful_san_count + 1))
                local file_size=$(stat -c%s "$src_path") # file size in bytes of the cleaned input playlist
                local extinf_count=$(grep -c "#EXTINF" "$src_path" || true) # count of #EXTINF lines present in the cleaned input playlist
                cleaned_details+="\n  - $src: $file_size bytes, $extinf_count #EXTINF lines"  # for appending to the summary list for the group-level debug echo
                echo "Cleaned input playlist $src (size: $file_size bytes). $extinf_count #EXTINF lines found." # debug for cleaning step for individual input playlists
              else
                echo "ERROR: Cleaning FAILED for input playlist $src or valid playlist $src NOT FOUND in $(pwd). Skipping file."
              fi
            done

            # Sub-step 1b: Echo group-level summary report after cleaning input files for this merged playlist
            if [ $successful_san_count -gt 0 ]; then
              echo -e "RESULT: Cleaning of input playlists for merged playlist $output COMPLETED. $successful_san_count input playlists successfully cleaned:$cleaned_details"
            fi

            # Sub-step 1c: Gate - if no input playlists for this merged playlist were cleaned, skip its merge process
            if [ $successful_san_count -eq 0 ]; then # gate: if none of the input playlists for this merged playlist were successfully cleaned, skips its merge process, workflow continues to cleaning imput playlists for the next merged playlist (ensured by 'return' rather than 'exit')
              echo "RESULT: Merge process for $output FAILED at cleaning stage. Found no valid input playlists or cleaning failed for all input playlists."
              return 1 # exits the process_group function with an error code; TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) logic used in calling the function ensures that the failure is recorded
            fi
 
            # Sub-step 1d: Deduplicate, format and append stream URLs from each cleaned input playlist to this merged playlist
            echo "Starting deduplication and merge of cleaned input playlists into $output..."
            (
              set -e # ensures that subshell exits on critical command failure
              local target="$dir/$output"
              local TRACKER="/tmp/seen_${output}_$(date +%s).tmp" # creates a unique temp tracker for this group's deduplication

              # module: initialize merged playlist
              echo "#EXTM3U" > "$target" # initializes the merged playlist with standard EXTM3U header
              printf "\n" >> "$target" # adds 1-line trailing spacing
              touch "$TRACKER" # initialize a fresh instance of the tracker at this point

              # module: append streams from a cleaned input playlist to the merged playlist after deduplication, custom spacing and formatting; echo result
              local block_index=0 # block_index tracks which source m3u-block we are currently writing to
              for src in "${sources[@]}"; do
                local src_path="$dir/$src"
                [ ! -f "$src_path" ] && continue
                RAW_DATE=$(stat -c %y "$src_path") # captures raw update date from the input playlist
                LAST_MOD=$(TZ='Asia/Kolkata' date -d "$RAW_DATE" "+%d-%m-%Y %H:%M IST") # formats raw date to DD-MM-YYYY format and applies IST timezone

                # function: awk for deduplication and custom formatting
                awk -v src_name="$src" -v tracker="$TRACKER" -v idx="$block_index" -v mod_date="$LAST_MOD" '

                  # sub-module: pre-processing
                  BEGIN {
                    count=0; tag_buf=""; file_buf=""; found_stream=0; # ignore everything till found_stream tuens 1, then start processing everything
                    if ((getline < tracker) > 0) { # pre-load disk tracker - load existing unique stream URLs into a memory array
                      do { seen[$0] = 1 } while ((getline < tracker) > 0)
                      close(tracker)
                    }
                  } 

                  # sub-module: deduplication
                  /^#EXTINF/ { found_stream=1 } # found_stream: 1 when the first #EXTINF line in the input playlist is found
                  {
                    if (found_stream == 0) next; # discard all lines before first #EXTINF is found, process everything thereon
                    if (/^#/) {
                        tag_buf = tag_buf $0 "\n" # buffer metadata lines
                    } else if (/^(http|https):\/\//) {
                        url = $0 # deduplicate URLs using the group tracker                       
                        if (!seen[url]) { # deduplication: internal memory check
                            print url >> tracker # update disk tracker
                            seen[url] = 1 # update memory array                      
                            file_buf = file_buf tag_buf url "\n"
                            count++
                        }
                        tag_buf = "" # reset metadata buffer for next stream URL
                    }
                  }

                  # sub-module: custom spacing and formatting
                  END {
                    if (count > 0) { # custom spacing logic: block_index 0 = first block (no preceding space), block_index > 0 = subsequent blocks (2-line preceding space)
                      prefix = (idx == 0) ? "" : "\n\n" # if not the first block, prepend 2 empty lines
                      printf "%s# Playlist from %s (last updated %s) : %d streams \n\n%s", prefix, src_name, mod_date, count, file_buf # add source identifier header + 1-line trail spacing + content
                    }
                  }                 
                ' "$src_path" >> "$target" # appends input playlists to the merged playlist using the awk logic

                if grep -q "Playlist from $src" "$target"; then # only increment block_index if this block was successfully added to the target
                   block_index=$((block_index + 1))
                fi        
                echo "Stream URLs from $src deduplicated and appended. $output now has $(wc -l < "$TRACKER") unique stream URLs."
              done # end of module for deduplication, custom spacing, formatting and appending streams
              
              # module: clean up tracker
              rm -f "$TRACKER"
            )

            # Sub-step 1e: Echo result at end of merge process for this merged playlist
            if [ $? -eq 0 ]; then
              local final_path="$dir/$output"
              local final_size=$(stat -c%s "$final_path")
              local final_extinf=$(grep -c "#EXTINF" "$final_path" || true)
              echo "RESULT: Merge process for $output COMPLETED. $output ($final_size bytes) is ready to push. $final_extinf #EXTINF lines present." # debug at the end of merge process
              return 0
            else
              echo "RESULT: Merge process for $output FAILED at deduplication, fomratting or appending stage." 
              return 1
            fi
          }

          # Sequential independent execution of playlist groups
          process_group "${{ env.DESTINATION_DIR1_PATH }}" "${{ env.MERGED_PLAYLIST1 }}" "sl-le-1.m3u" "sl-le-2.m3u" "sl-le-3.m3u" || TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1))
          process_group "${{ env.DESTINATION_DIR2_PATH }}" "${{ env.MERGED_PLAYLIST2 }}" "fc-le-1.m3u" "fc-le-2.m3u" "fc-le-3.m3u" "fc-le-4.m3u" || TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1))

          echo "TOTAL_FAILURES=$TOTAL_GROUP_FAILURES" >> $GITHUB_ENV # export result to GitHub environment

      - name: Commit Changes and Push Merged Playlists to Destination Branch
        run: |
          cd destination-branch

          # Step 1: Configure git             
          git config url.https://${{ secrets.PAT_FG_SYNC }}@github.com/.insteadOf https://github.com/
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Step 2: Define the list of merged playlists for analysis
          FILES=(
            "${{ env.DESTINATION_DIR1_PATH }}/${{ env.MERGED_PLAYLIST1 }}"
            "${{ env.DESTINATION_DIR2_PATH }}/${{ env.MERGED_PLAYLIST2 }}"
          )

          # Step 3: Iterate through the merged playlists to analyse using tracking variables for the final summary echos        
          COMMITTED_NAMES=""
          COMMITTED_COUNT=0
          SKIPPED_NAMES=""
          SKIPPED_COUNT=0
          SUMMARY_DETAILS=""         
          for FILE in "${FILES[@]}"; do
            FILE_NAME=$(basename "$FILE")
            if [ -f "$FILE" ]; then       
              GIT_STATUS=$(git status --porcelain "$FILE") # uses git status to determine the file state
              if [[ -z "$GIT_STATUS" ]]; then # if git status is empty, it means the file is unchanged/clean
                SKIPPED_NAMES+="$FILE_NAME " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for merged playlist $FILE_NAME."
              else
                COMMITTED_NAMES+="$FILE_NAME " # playlist is changed or new and will be committed; adds to committed list and gathers metrics
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SIZE=$(stat -c%s "$FILE") 
                EXTINF=$(grep -c '#EXTINF' "$FILE" || true)
                SUMMARY_DETAILS+="\n  - $FILE_NAME ($SIZE bytes): now has $EXTINF #EXTINF lines"
                git add "$FILE" # stage only the changed/new files selectively based on the git status check         
              fi
            else
              echo "WARNING: Merged playlist $FILE_NAME not found."
            fi
          done

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "----------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit for any merged playlists"
          else
            git commit -m "Auto-updated merged playlists" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DESTINATION_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DESTINATION_BRANCH }} # pushes to the destination branch
            echo "----------------------------------------------------------------------------------------------------------"
            echo "RESULT: Commits were made to $COMMITTED_COUNT merged playlist(s): $COMMITTED_NAMES"
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "----------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT merged playlist(s) were found unchanged and were skipped: $SKIPPED_NAMES"
          echo "----------------------------------------------------------------------------------------------------------"
          if [ "${{ env.TOTAL_FAILURES }}" -gt 0 ]; then # final job reporting
            echo "DEBUG: Final Status: FAIL (total $TOTAL_FAILURES group failures recorded)"
            exit 1
          fi



  print_trigger_message: # echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
    runs-on: ubuntu-latest
    steps:
      - name: Print Trigger Message
        run: |
          echo "This workflow run was triggered by: ${{ github.event.inputs.message }}"
