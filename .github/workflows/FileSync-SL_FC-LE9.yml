# This workflow is configired to be trigerred manually from GitHUb WebUI or by an external cronjob scheduler service
# This GitHub Action has 3 jobs:
# 1) copies specified playlists from source repo and overwrites corresponding specified playlists in destination branch/directory in destination repo
# 2) creates a custom-formatted deduplicated merged playlist with prioritized m3u comtent-blocks from already copied source playlists
# 3) echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
# Works for copying from any branch of a public source repo into any branch of a public/private destination repo
# The .yml file may be placed in any 3rd public/private repo (non-source, non-destination)(say, control repo) in its default (i.e. main) branch at the 'github/workflows/' directory
# Need to use Personal Access Token (PAT) for commiting and pushing to destination repo - create a PAT Classic (with workflow permissions) or PAT Fine-grained (with actions(R/W), contents(code)(R/W), pull requests(R/W) and workflows(R/W) permissions)
# Store PAT Classic or PAT Fine-grained token's value in the control repo's secrets with names PAT_CL_SYNC or PAT_FG_SYNC

name: FileSync SL&FC LE 9

on: 
  workflow_dispatch: # allows manual workflow run from the Actions tab (workflow must be in default branch to work)
    inputs: # optional input(s) that the cronjob service can pass on to the workflow
      message:
        description: 'source trigger of the workflow run'
        required: true
        default: 'manual GitHub WebUI trigger' # default message to be printed if workflow is triggered manually via WebUI or if there is no input message alongside a trigger by cronjob service
        type: string # optionally, specifies type of message
  # internal scehduled run parameters in the 2 lines below 'commented out' as external cronjob scheduler service is engaged
  # schedule: # scheduled to run at designated times or every x minutes (minimum interval allowed is 5 minutes)
    # - cron: '*/22 * * * *'  
      
env:
  CRONJOB_SERVICE: "FastCron"
  # Destination Repo Configuration
  DESTINATION_REPO_URL: "https://github.com/AmSin1/Repo.git"
  DESTINATION_REPO: "AmSin1/Repo"
  DESTINATION_BRANCH: "autocopied"
  # Group 1 : Source URLs, Directory Path and Merged Playlist
  DESTINATION_DIR1_PATH: "sl-le"
  MERGED_PLAYLIST_sl_le: "sl-le-all.m3u"
  SOURCE_PLAYLIST1a_URL: "https://github.com/doctor-8trange/zyphora/raw/main/data/sony.m3u" # source updated and working
  SOURCE_PLAYLIST1b_URL: "https://github.com/drmlive/sliv-live-events/raw/main/sonyliv.m3u" # sporce updated and working
  SOURCE_PLAYLIST1c_URL: "https://github.com/abusaeeidx/SonyLiv-Event-Auto-update/raw/main/sonyliv.m3u" # source NOT updated
  SOURCE_PLAYLIST1d_URL:
  # Group 2 : Source URLs, Directory Path and Merged Playlist
  DESTINATION_DIR2_PATH: "fc-le"
  MERGED_PLAYLIST_fc_le: "fc-le-all.m3u"
  SOURCE_PLAYLIST2a_URL: "https://github.com/jitendra-unatti/fancode/raw/main/data/fancode.m3u" # source updated and working
  SOURCE_PLAYLIST2b_URL: "https://github.com/drmlive/fancode-live-events/raw/main/fancode.m3u" # source updated and working
  SOURCE_PLAYLIST2c_URL: "https://github.com/IPTVFlixBD/Fancode-BD/raw/main/playlist.m3u" # source updated and working
  SOURCE_PLAYLIST2d_URL: "https://github.com/abusaeeidx/CrestSport-IPTV-Collection/raw/main/fncbd.m3u" # source updated and working
  SOURCE_PLAYLIST2e_URL:

concurrency: # ensures that only one GitHub workflow runs at a time for a specific branch in a repo, prevents conditions where two separate workflow runs attempt to push to same branch simultaneously
  group: sync-${{ github.head_ref || github.ref_name }}
  cancel-in-progress: false # 'false' ensures that a newly-started workflow runs sequentially after and does not terminate a still in-progress workflow run, ensuring that every workflow's changes are eventually pushed

jobs:   
  setup_playlist_matrix: # setup: calculate dynamic playlist matrix
    runs-on: ubuntu-latest
    outputs:
      batch_indices: ${{ steps.set-matrix.outputs.indices }} # dynamic matrix in json to pass indices to the subsequent job(s) to launch parallel processing runners via GitHub Action
    steps:
      - id: set-matrix
        run: |
          # Step 1: Define the playlist array (each line: URL|DIRECTORY|FILENAME)
          LIST=$(cat <<EOF
          ${{ env.SOURCE_PLAYLIST1a_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-1.m3u
          ${{ env.SOURCE_PLAYLIST1b_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-2.m3u
          ${{ env.SOURCE_PLAYLIST1c_URL }}|${{ env.DESTINATION_DIR1_PATH }}|sl-le-3.m3u
          ${{ env.SOURCE_PLAYLIST2a_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-1.m3u
          ${{ env.SOURCE_PLAYLIST2b_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-2.m3u
          ${{ env.SOURCE_PLAYLIST2c_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-3.m3u
          ${{ env.SOURCE_PLAYLIST2d_URL }}|${{ env.DESTINATION_DIR2_PATH }}|fc-le-4.m3u
          EOF
          )

          # Step 2: Count number of playlists and calculate batches
          TOTAL=$(echo "$LIST" | grep -c '[^[:space:]]')
          BATCH_SIZE=30 # number of playlists per batch for parallel processing
          NUM_BATCHES=$(( (TOTAL + BATCH_SIZE - 1) / BATCH_SIZE )) # calculates required number of parallel runners
          
          # Step 3: Create the JSON array output for subsequent jobs
          INDICES=$(seq 0 $((NUM_BATCHES - 1)) | jq -R . | jq -s -c .) # generates a JSON array [0, 1, 2...] for the matrix strategy
          echo "indices=$INDICES" >> "$GITHUB_OUTPUT"
          
          # Step 4: Store the playlist array in a file for use by subsequent job(s)
          echo "$LIST" > playlist_array.txt
          echo "DEBUG: Playlist(s) to be downloaded: $TOTAL. Playlists per batch set at $BATCH_SIZE. Parallel runner(s) required: $NUM_BATCHES."

      - name: Upload Artifact with the Playlist Array
        uses: actions/upload-artifact@v4
        with:
          name: playlist-array
          path: ${{ github.workspace }}/playlist_array.txt
          if-no-files-found: error # forces this job to fail if the file is missing



  download_playlists: # copies specified playlists from source repos and overwrites corresponding specified playlists in destination branch/directories in destination repo
    needs: setup_playlist_matrix # forces this job to wait for the referenced job to finish (forces sequential instead of the default parallel execution)
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    strategy:
      fail-fast: false # ensures one failed playlist doesn't stop others in the batch
      matrix:
        batch: ${{ fromJson(needs.setup_playlist_matrix.outputs.batch_indices) }}       
    steps:
      - name: Download Artifact with the Playlist Array
        uses: actions/download-artifact@v4
        with:
          name: playlist-array
          path: ${{ github.workspace }}

      - name: Download Playlists from Source Repos
        run: |     
          # Step 1: Load the playlist array into a Bash array for slicing
          mapfile -t PLAYLISTS < playlist_array.txt # load the list from the downloaded artifact into Bash array

          # Step 2: Batch process the playlist array
          BATCH_SIZE=30
          START=$(( ${{ matrix.batch }} * BATCH_SIZE ))
          END=$(( START + BATCH_SIZE - 1 ))

          for i in $(seq $START $END); do
            entry="${PLAYLISTS[$i]}"
            [ -z "$entry" ] && continue # boundary check for uneven batches

            IFS="|" read -r URL DIR FILE <<< "$entry" # extracts data from the playlist array using IFS pipe separator
            TARGET="downloads/$DIR/$FILE" # sets temporary destination file path for each playlist download
            mkdir -p "downloads/$DIR" # creates destination directory if it doesn't already exist

            # Sub-step 2a: Download playlists from source repos
            CURL_FLAGS="--connect-timeout 3 --retry 3 --retry-delay 1 --retry-max-time 10 --retry-all-errors --no-progress-meter" # optional curl flags
            HTTP_CODE=$(curl $CURL_FLAGS -R -L --fail \
              -H "X-GitHub-Api-Version: 2022-11-28" \
              -H "User-Agent: FileSync-Batch-2026" \
              -o "$TARGET" -w "%{http_code}" "$URL" || echo "000") # downloads the playlist using curl and captures the HTTP status code to verify download success

            # Sub-step 2b: Fetch and sync remote commit timestamp, and echo success/failure with key metrics for each download
            if [ "$HTTP_CODE" = "200" ] && [ -f "$TARGET" ]; then # checks if playlist exists AND was actually downloaded during this specific workflow run
         
              # module: fetch remote commit timestamps
              REMOTE_DATE="" # starts afresh for this iteration
              TIME_NOTE="(standard download)" # default value
              if [[ "$URL" == *"github.com"* ]] || [[ "$URL" == *"githubusercontent.com"* ]]; then
            
                # sub-module: for downloads from GitHub URLs, capture remote commit timestamps from GitHub API 
                OWNER_REPO=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/([^/]+/[^/]+).*|\2|') # extracts 'owner/repo' to call the GitHub API
                FILE_PATH=$(echo "$URL" | sed -E 's|https://(github.com\|raw.githubusercontent.com)/[^/]+/[^/]+/(blob/\|raw/)?([^/]+/)?(.*)|\4|') # extracts 'path/to/file' to call the GitHub API
                API_URL="https://api.github.com/repos/${OWNER_REPO}/commits?path=${FILE_PATH}&per_page=1" # sets the URL to target the playlist's commit history
                REMOTE_DATE=$(curl -s \
                  -H "Authorization: Bearer ${{ secrets.PAT_FG_SYNC }}" \
                  -H "X-GitHub-Api-Version: 2022-11-28" \
                  -H "User-Agent: GitHub-Actions-FileSync" \
                  "$API_URL" | jq -r '.[0].commit.committer.date' 2>/dev/null || echo "") # captures the remote commit timestamp by querying GitHub API
                [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ] && TIME_NOTE="([✓]API verified)" # updates the timestamp tag
              else

                # sub-module: for downloads from non-GitHub URLs, fallback to capturing remote commit timestamps from HTTP header   
                HEADER_VAL=$(curl -sI "$URL" | grep -i 'Last-Modified:' | cut -d':' -f2- | xargs) # captures the remote commit timestamp from the remote file's header metadata
                if [ -n "$HEADER_VAL" ]; then
                  REMOTE_DATE=$(date -d "$HEADER_VAL" -u +"%Y-%m-%dT%H:%M:%SZ" 2>/dev/null)
                  TIME_NOTE="([✓]header verified)" # updates the timestamp tag
                fi
              fi

              # module: sync remote commit timestamps
              if [ -n "$REMOTE_DATE" ] && [ "$REMOTE_DATE" != "null" ]; then
                touch -d "$REMOTE_DATE" "$TARGET" # manually syncs the downloaded file's timestamp to match remote commit date
                MOD_TIME=$(TZ="Asia/Kolkata" date -d "$REMOTE_DATE" "+%d-%m-%Y %H:%M IST") # formats and converts timestamp to IST
              else
                MOD_TIME=$(TZ="Asia/Kolkata" date -r "$TARGET" "+%d-%m-%Y %H:%M IST") # fallback to downloaded playlist's local commit (workflow run) timestamp in case fetching of remote timestamp via GitHub API or HTTP header fails
                TIME_NOTE="([x]download time)" # updates the timestamp tag
              fi

              # module: echo success/failure of each downloaded playlist with key metrics
              SIZE=$(stat -c%s "$TARGET") # calculates file size of downloaded playlist
              COUNT=$(grep -c '#EXTINF' "$TARGET" || true) # counts number of #EXTINF lines in downloaded playlist
              echo "$FILE download : SUCCESS. Last updated $MOD_TIME $TIME_NOTE. $SIZE bytes, $COUNT #EXTINF lines."
              echo "$DIR|$FILE|$SIZE|$COUNT|$MOD_TIME|$TIME_NOTE" >> "batch_${{ matrix.batch }}.meta" # record each playlist's result to metadata fragment (format: DIR|FILE|SIZE|COUNT|TIMESTAMP|TIMENOTE)   
            else
              echo "$FILE download : FAILED (HTTP: $HTTP_CODE)"
              echo "$DIR|$FILE|FAILED|0|N/A|N/A" >> "batch_${{ matrix.batch }}.meta"
            fi
            unset MOD_TIME # clears for next iteration
          done

      - name: Upload Artifact with Metadata Fragments
        if: always() # ensures metadata is uploaded even if some downloads in the batch fail
        uses: actions/upload-artifact@v4
        with:
          name: meta-batch-${{ matrix.batch }}
          path: |
            downloads/
            batch_${{ matrix.batch }}.meta



  consolidate_and_push: # create consolidated metadata file, echo group metrics, commit and push to destination branch
    needs: download_playlists
    if: always() # this job runs regardless of other jobs' success/failure status, generates the final report even if some downloads failed
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Destination Branch in Destination Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DESTINATION_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DESTINATION_BRANCH }}  # set the destination branch to checkout
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'destination-branch'

      - name: Download Artifact with All Metadata Fragments
        uses: actions/download-artifact@v4
        with:
          pattern: meta-batch-* # collects all parallel metadata pieces
          merge-multiple: true # flattens all fragments and reconstructs in the current folder
          path: .

      - name: Create Consolidated Metadata File and Echo Group-Level Download Results
        run: |
          # Step 1: Combine all metadata fragments of individual playlists into a consolidated metadata file
          cat batch_*.meta > remote_timestamps.meta # consolidated metadata file has the format: DIR|FILE|SIZE|COUNT|TIMESTAMP|TIMENOTE
          
          # Step 2: Echo group-level download results
          DIRS=$(cut -d'|' -f1 remote_timestamps.meta | sort -u) # extract unique directories from the consolidated metadata file
          echo "------------------------------------------------------------------------------------------------------------------------"
          for DIR in $DIRS; do
            S_COUNT=0; T_EXTINF=0; T_SIZE=0; F_COUNT=0; F_NAMES=""
            
            # Sub-step 1a: Aggregate group-level download stats for this specific folder
            while IFS="|" read -r f_dir f_name f_size f_extinf f_time f_note; do
              if [ "$f_size" = "FAILED" ]; then
                F_COUNT=$((F_COUNT + 1)) # count of playlists for which download failed 
                F_NAMES+="$f_name " # names of playlists for which download failed 
              else
                S_COUNT=$((S_COUNT + 1)) # count of playlists for which download succeeded
                T_EXTINF=$((T_EXTINF + f_extinf)) # count of #EXTINF lines in playlists for which download succeeded
                T_SIZE=$((T_SIZE + f_size)) # total size of playlists for which download succeeded
              fi
            done < <(grep "^$DIR|" remote_timestamps.meta)
            
            # Sub-step 1b: Echo group-level download results with key metrics
            echo "RESULT for folder $DIR : successfully downloaded $S_COUNT playlists ($T_SIZE bytes). Total $T_EXTINF #EXTINF lines present."
            echo "RESULT for folder $DIR : failed to download $F_COUNT playlists: ${F_NAMES:-(none)}"
            echo "------------------------------------------------------------------------------------------------------------------------"
          done

      - name: Commit Changes and Push to Destination Branch
        run: |
          # Step 1: Move downloaded playlists to destination branch
          ls -R downloads/ # checks if the temporary folder exists
          cp -r downloads/* destination-branch/ # moves the downloaded playlists from the temp folder to the destination branch

          # Step 2: Enter destination directory and cleanup temporary files
          cd destination-branch # changes current working directory to the destination branch
          rm -rf ../downloads ../batch_*.meta # cleanup the temporary download structure and any individual batch metadata before git add     

          # Step 3: Configure git          
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG_SYNC }}@github.com/${{ env.DESTINATION_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
          
          # Step 4: Analyse downloaded playlists for final summary echos, and stage changed/new files for commit 
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""
          while IFS="|" read -r DIR FILE SIZE COUNT TIMESTAMP NOTE; do
            TARGET="$DIR/$FILE"
            if [ -f "$TARGET" ]; then
              if [[ -n $(git status --porcelain "$TARGET") ]]; then # uses git status to determine the file state
                COMMITTED_NAMES+="$FILE " # playlist is changed or new and will be committed; adds to committed list
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SUMMARY_DETAILS+="\n  - $FILE : last updated $TIMESTAMP $NOTE - $SIZE bytes, $COUNT #EXTINF lines"
                git add "$TARGET" # stages only the changed/new files selectively based on the git status check 
              else
                SKIPPED_NAMES+="$FILE " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for playlist $FILE."
              fi
            fi
          done < ../remote_timestamps.meta

          # Step 5: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit in any of the $SKIPPED_COUNT playlists. Skipping push."
          else
            git commit -m "Automated sync from source" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DESTINATION_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DESTINATION_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Successfully committed and pushed $COMMITTED_COUNT updated playlist(s)."
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT playlist(s) were found unchanged and were skipped: ${SKIPPED_NAMES:-(none)}"
          echo "------------------------------------------------------------------------------------------------------------------------"

      - name: Upload Artifcat with Remote Timestamps Metadata
        uses: actions/upload-artifact@v4
        with:
          name: playlist-metadata
          path: ${{ github.workspace }}/remote_timestamps.meta # final source-of-truth for all remote commit timestamps
          if-no-files-found: error # forces this job to fail if the file is missing


  
  create_merged_playlists: # creates custom-formatted deduplicated merged playlists with prioritized m3u comtent-blocks from input playlists
    needs: consolidate_and_push # forces this job to wait for the referenced job to finish (forces sequential instead of the default parallel execution)
    if: always() # this job runs regardless of other jobs' success/failure status
    runs-on: ubuntu-latest
    permissions:
      contents: write # grants permissions for the job to write to contents
    steps:          
      - name: Checkout Destination Branch in Destination Repo
        uses: actions/checkout@v4
        with:
          repository: '${{ env.DESTINATION_REPO }}' # specify 'owner/repo' of the destination repo
          ref: ${{ env.DESTINATION_BRANCH }}  # set the destination branch to checkout
          fetch-depth: 0 # get full history to allow rebasing (recommended for every job that needs to commit and push back)
          persist-credentials: false # REQUIRED if upstream repo is private (see wiki)
          path: 'destination-branch'

      - name: Download Artifcat with Remote Timestamps Metadata
        uses: actions/download-artifact@v4
        with:
          name: playlist-metadata 
          path: ${{ github.workspace }}

      - name: Process, Deduplicate, Format and Merge Playlists
        run: |
          cd destination-branch # changes current working directory to the destination branch
          TOTAL_GROUP_FAILURES=0 # tracks global failures across the clean and merge processes of all merged playlists, starts from 0
          PLAYLIST_METADATA="${{ github.workspace }}/remote_timestamps.meta"
          touch ../merged_files_to_commit.txt # creates a registry file to track successfully created merged playlists for the commit step

          # Step 1: Use helper function for group-wise cleaning and merging
          process_group() { # function encapsulating cleaning and merging processes for any given merged playlist folder
            local dir="$1"
            local output="$2"
            shift 2
            local sources=("$@")
            
            echo "-----------------------------------------------------------------------------"
            echo "PROCESS START: Creation of merged playlist $output in folder $dir"
            echo "-----------------------------------------------------------------------------"

            # Sub-step 1a: Clean input playlists for a specific merged playlist
            echo "Starting cleaning of input playlists for merged playlist $output..."
            local successful_san_count=0 # count of successfully cleaned input playlists for this merged playlist, starts from 0
            local cleaned_details="" # record of file-size and count of #EXTINF lines in successfully cleaned input playlists, starts blank

            for src in "${sources[@]}"; do
              local src_path="$dir/$src"

              # module: fallback to playlist in repository if current download of an input playlist fails 
              if [ ! -f "$src_path" ]; then # if input playlist is missing (current download failed), uses the existing playlist in the repo for merge process
                if [ -f "$src" ]; then
                  echo "FALLBACK: Current download not found for playlist $src. Using existing previous instance from repository."
                  mkdir -p "$dir"
                  cp "$src" "$src_path" # pulls the playlist from repo root into processing directory
                else
                  echo "SKIPPED: Download failed for playlist $src AND no previous instance found in repository. Skipping file."
                  continue
                fi
              fi

              # module: fetch remote timestamp and time-note for the input playlist from the consolidated metadata file
              local meta_entry=$(grep "|$src|" "$PLAYLIST_METADATA" || echo "")
              local LAST_MOD="" # initializes file to capture remote timestamps and time-notes from the consolidated metadata file
              if [[ -n "$meta_entry" && "$(echo "$meta_entry" | cut -d'|' -f3)" != "FAILED" ]]; then   
                # sub-module: for input playlist successfully downloaded in this workflow run, extract from condolidated metadata file
                local ts=$(echo "$meta_entry" | cut -d'|' -f5) # extracts field 5 (remote timestamp) 
                local note=$(echo "$meta_entry" | cut -d'|' -f6) # extracts field 6 (time note)
                LAST_MOD="$ts $note" # combines remote timestamp and time note
              else
                # sub-module: for input playlist not downloaded in this worflow run, fallback to timestamp of the existing file in the repo
                local ts=$(TZ='Asia/Kolkata' date -r "$src_path" "+%d-%m-%Y %H:%M IST")
                LAST_MOD="$ts ([x]repo fallback)"
              fi

              # module: clean input playlist (remove carriage returns and BOM) and echo its success/failure metrics
              if (set -e; sed -i '1s/^\xef\xbb\xbf//; s/\r//g' "$src_path"); then # strips Windows line endings (\r) and any non-standard character markers
                successful_san_count=$((successful_san_count + 1))
                local file_size=$(stat -c%s "$src_path") # file size in bytes of the cleaned input playlist
                local extinf_count=$(grep -c "#EXTINF" "$src_path" || true) # count of #EXTINF lines present in the cleaned input playlist
                cleaned_details+="\n  - $src: last updated $LAST_MOD, $file_size bytes, $extinf_count #EXTINF lines"  # for appending to the summary list for the group-level debug echo
                echo "Cleaned input playlist $src (size: $file_size bytes). $extinf_count #EXTINF lines found." # debug for cleaning step for individual input playlists
              else
                echo "ERROR: Cleaning FAILED for input playlist $src or valid playlist $src NOT FOUND. Skipping file."
              fi
            done

            # Sub-step 1b: Echo group-level summary report after cleaning input files for this merged playlist
            if [ $successful_san_count -gt 0 ]; then
              echo -e "RESULT: Cleaning of input playlists for merged playlist $output COMPLETED. $successful_san_count input playlists successfully cleaned:$cleaned_details"
            fi

            # Sub-step 1c: Gate - if no input playlists for this merged playlist were cleaned, skip its merge process
            if [ $successful_san_count -eq 0 ]; then # gate: if none of the input playlists for this merged playlist were successfully cleaned, skips its merge process, workflow continues to cleaning imput playlists for the next merged playlist (ensured by 'return' rather than 'exit')
              echo "RESULT: Merge process for $output FAILED at cleaning stage. Found no valid input playlists or cleaning failed for all input playlists."
              return 1 # exits the process_group function with an   error code; TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) logic used in calling the function ensures that the failure is recorded
            fi
 
            # Sub-step 1d: Deduplicate, format and append stream URLs from each cleaned input playlist to this merged playlist
            echo "Starting deduplication and merge of cleaned input playlists into $output..."
            (
              set -e # ensures that subshell exits on critical command failure
              local target="$dir/$output"
              local TRACKER="/tmp/seen_${output}.tmp" # creates a unique temp tracker for this group's deduplication

              # module: initialize merged playlist
              echo "#EXTM3U" > "$target" # initializes the merged playlist with standard EXTM3U header
              printf "\n" >> "$target" # adds 1-line trailing spacing
              touch "$TRACKER" # initialize a fresh instance of the tracker at this point

              # module: append streams from a cleaned input playlist to the merged playlist after deduplication, custom spacing and formatting; echo result
              local block_index=0 # block_index tracks which source m3u-block we are currently writing to
              for src in "${sources[@]}"; do
                local src_path="$dir/$src"
                [ ! -f "$src_path" ] && continue

                # sub-module: fetch-timestamps for the block-level header (similar logic to cleaning stage)
                local meta_entry=$(grep "|$src|" "$PLAYLIST_METADATA" || echo "")
                local LAST_MOD=""
                if [[ -n "$meta_entry" && "$(echo "$meta_entry" | cut -d'|' -f3)" != "FAILED" ]]; then
                  LAST_MOD="$(echo "$meta_entry" | cut -d'|' -f5) $(echo "$meta_entry" | cut -d'|' -f6)"
                else
                  LAST_MOD="$(TZ='Asia/Kolkata' date -r "$src_path" '+%d-%m-%Y %H:%M IST') ([x]repo fallback)"
                fi

                # sub-module: awk function for deduplication and custom formatting
                awk -v src_name="$src" -v tracker="$TRACKER" -v idx="$block_index" -v mod_date="$LAST_MOD" '

                  # pre-processing
                  BEGIN {
                    count=0; tag_buf=""; file_buf=""; found_stream=0; # ignore everything till found_stream tuens 1, then start processing everything
                    if ((getline < tracker) > 0) { # pre-load disk tracker - load existing unique stream URLs into a memory array
                      do { seen[$0] = 1 } while ((getline < tracker) > 0)
                      close(tracker)
                    }
                  } 

                  # deduplication
                  /^#EXTINF/ { found_stream=1 } # found_stream: 1 when the first #EXTINF line in the input playlist is found
                  {
                    if (found_stream == 0) next; # discard all lines before first #EXTINF is found, process everything thereon
                    if (/^#/) {
                        tag_buf = tag_buf $0 "\n" # buffer metadata lines
                    } else if (/^(http|https):\/\//) {
                        url = $0 # deduplicate URLs using the group tracker                       
                        if (!seen[url]) { # deduplication: internal memory check
                            print url >> tracker #   update disk tracker
                            seen[url] = 1 # update memory array                      
                            file_buf = file_buf tag_buf url "\n"
                            count++
                        }
                        tag_buf = "" # reset metadata buffer for next stream URL
                    }
                  }

                  # custom spacing and formatting
                  END {
                    if (count > 0) { # custom spacing logic: block_index 0 = first block (no preceding space), block_index > 0 = subsequent blocks (2-line preceding space)
                      prefix = (idx == 0) ? "" : "\n\n" # if not the first block, prepend 2 empty lines
                      printf "%s# Playlist from %s : %d stream(s) \n", prefix, src_name, count # add source identifier header
                      printf "# (last updated %s) \n\n%s", mod_date, file_buf # add remote update timestamp for the source playlist + 1-line trail spacing + content
                    }
                  }                 
                ' "$src_path" >> "$target" # appends input playlists to the merged playlist using the awk logic

                if grep -q "Playlist from $src" "$target"; then # only increment block_index if this block was successfully added to the target
                   block_index=$((block_index + 1))
                fi        
                echo "Stream URLs from $src deduplicated and appended. $output now has $(wc -l < "$TRACKER") unique stream URLs."
              done # end of module for deduplication, custom spacing, formatting and appending streams
              
              # module: clean up tracker
              rm -f "$TRACKER"
            )

            # Sub-step 1e: Echo result at end of merge process for this merged playlist
            if [ $? -eq 0 ]; then
              local final_path="$dir/$output"
              local final_size=$(stat -c%s "$final_path")
              local final_extinf=$(grep -c "#EXTINF" "$final_path" || true)
              echo "RESULT: Merge process for $output COMPLETED. $output ($final_size bytes) is ready to push. $final_extinf #EXTINF lines present." # debug at the end of merge process
              return 0
            else
              echo "RESULT: Merge process for $output FAILED at deduplication, fomratting or appending stage." 
              return 1
            fi
          }

          # Step 2: Dynamic execution loop (using indirect expansion) for playlist groups
          DIRS=$(cut -d'|' -f1 "$PLAYLIST_METADATA" | sort -u) # extracts all unique directories in the consolidated metadata file
          for DIR in $DIRS; do # maps the identified directories to merged playlist names
              VAR_SUFFIX=$(echo "$DIR" | tr '-' '_') # creates suffix for the lookup variable from directory name (e.g., sl-le -> sl_le)
              LOOKUP_VAR="MERGED_PLAYLIST_$VAR_SUFFIX" # constructs the lookup variable (e.g., sl-le -> MERGED_PLAYLIST_sl_le)
              OUTPUT_NAME="${!LOOKUP_VAR}" # indirect expansion to get the variable value (e.g., "sl-le-all.m3u")
              if [ -z "$OUTPUT_NAME" ]; then # skips the folder (e.g., sl-le) if its merged playlist variable (e.g., MERGED_PLAYLIST_sl_le) hasn't been defined in the 'env:'
                echo "DEBUG: Skipping group $DIR - no 'env.$LOOKUP_VAR' defined."
                continue
              fi
              mapfile -t FILES_FOR_GROUP < <(grep "^$DIR|" "$PLAYLIST_METADATA" | cut -d'|' -f2) # extracts only the input playlist names belonging to current group, and puts these filenames into an array
              if [ ${#FILES_FOR_GROUP[@]} -gt 0 ]; then # checks if any input playlists for this group were actually found
                  if process_group "$DIR" "$OUTPUT_NAME" "${FILES_FOR_GROUP[@]}"; then # calls the 'process_group' function with the dynamic folder, merged playlist name, and input playlist array
                      echo "$DIR/$OUTPUT_NAME" >> ../merged_files_to_commit.txt # registers the successfully merged playlist for the commit step
                  else
                      TOTAL_GROUP_FAILURES=$((TOTAL_GROUP_FAILURES + 1)) # increments the global error counter everytime the merge function fails 
                  fi
              fi
          done             
          
          # Step 3: Export failure count to GitHub environment for final job status
          echo "TOTAL_FAILURES=$TOTAL_GROUP_FAILURES" >> $GITHUB_ENV 

      - name: Commit Changes and Push Merged Playlists to Destination Branch
        run: |
          cd destination-branch

          # Step 1: Configure git             
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin https://x-access-token:${{ secrets.PAT_FG_SYNC }}@github.com/${{ env.DESTINATION_REPO }}.git # avoids the alternative 'insteadOf' approach to prevent leakage of secret in logs
  
          # Step 2: Dynamically load the merged playlists registered in the previous step
          if [ -f "../merged_files_to_commit.txt" ]; then
            mapfile -t FILES < ../merged_files_to_commit.txt
          else
            FILES=()
          fi

          # Step 3: Iterate through the merged playlists to analyse using tracking variables for the final summary echos        
          COMMITTED_NAMES=""; COMMITTED_COUNT=0; SKIPPED_NAMES=""; SKIPPED_COUNT=0; SUMMARY_DETAILS=""         
          for FILE in "${FILES[@]}"; do
            FILE_NAME=$(basename "$FILE")
            if [ -f "$FILE" ]; then       
              GIT_STATUS=$(git status --porcelain "$FILE") # uses git status to determine the file state
              if [[ -z "$GIT_STATUS" ]]; then # if git status is empty, it means the file is unchanged/clean
                SKIPPED_NAMES+="$FILE_NAME " # playlist is identical to the one in the repo and will not be committed; adds to skipped list
                SKIPPED_COUNT=$((SKIPPED_COUNT + 1))
                echo "RESULT: No changes found to commit for merged playlist $FILE_NAME."
              else
                COMMITTED_NAMES+="$FILE_NAME " # playlist is changed or new and will be committed; adds to committed list and gathers metrics
                COMMITTED_COUNT=$((COMMITTED_COUNT + 1))
                SIZE=$(stat -c%s "$FILE") 
                EXTINF=$(grep -c '#EXTINF' "$FILE" || true)
                SUMMARY_DETAILS+="\n  - $FILE_NAME ($SIZE bytes): now has $EXTINF #EXTINF lines"
                git add "$FILE" # stage only the changed/new files selectively based on the git status check         
              fi
            else
              echo "WARNING: Merged playlist $FILE_NAME not found."
            fi
          done

          # Step 4: Commit changes, push to destination branch and echo final commit summaries  
          if git diff --cached --quiet; then # checks if there are any changes
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: No changes found to commit for any merged playlists"
          else
            git commit -m "Auto-updated merged playlists" # only commits if a binary or text difference exists, alongwith commit message
            # git pull origin ${{ env.DESTINATION_BRANCH }} --rebase # rebases pull to ensure local is ahead of remote (enable if facing commit/push failures)
            git push origin ${{ env.DESTINATION_BRANCH }} # pushes to the destination branch
            echo "------------------------------------------------------------------------------------------------------------------------"
            echo "RESULT: Commits were made to $COMMITTED_COUNT merged playlist(s): $COMMITTED_NAMES"
            echo -e "Details: $SUMMARY_DETAILS"
          fi
          echo "------------------------------------------------------------------------------------------------------------------------"
          echo "RESULT: $SKIPPED_COUNT merged playlist(s) were found unchanged and were skipped: $SKIPPED_NAMES"
          echo "------------------------------------------------------------------------------------------------------------------------"
          
          # Step 5: Final job failure reporting         
          if [ "${{ env.TOTAL_FAILURES }}" -gt 0 ]; then
            echo "DEBUG: Final Status: FAIL (workflow ended with $TOTAL_FAILURES group failures)"
            exit 1
          fi



  print_trigger_message: # echos the input message identifying source of workflow trigger (manual via WebUI or automated external cronjob service)
    runs-on: ubuntu-latest
    steps:
      - name: Print Trigger Message
        run: |
          echo "This workflow run was triggered by: ${{ github.event.inputs.message }}"
